num_experiments: 1

########## DATA
data_dir: "datasets/cfpb/"
prefix: "final_data.tsv"
train_suffix: ".train"
dev_suffix: ".dev"
test_suffix: ".test"
max_seq_len: 60

sos: "<s>"
eos: "</s>"
unk: "<unk>"


# NOTE -- var names can't overlap with features
data_spec:
  - name: "consumer-complaint"
    type: "text"

  - name: "product-in-question"
    type: "categorical"
    control: True
    skip: False
    weight: 1

  - name: "issue-in-question"
    type: "categorical"
    control: True
    skip: False
    weight: 1

  - name: "state-of-origin"
    type: "categorical"
    control: True
    skip: True
    weight: 1

  - name: "company-response"
    type: "categorical"
    control: False
    skip: True
    weight: 1

  - name: "timely-response"
    type: "categorical"
    control: False
    skip: False
    weight: 1




########## MODELING
seed: 3
working_dir: "test_run"

# NOTE: can be randomized
vocab:
  # points to a 1-per-line file of tokens, or null (generate with top_n words)
  vocab_file: null 
  top_n: 1000
  preselection_algo: 'identity' # odds-ratio, mutual-information, identity
  preselection_features: 500 # set to 0 to turn pre-selection off



# NOTE: params can be randomized
# all of these will be run
model_spec:
  - type: "neural"
    name: "flip_1"
    skip: False
    params:
      use_glove: True. # True, False
      attn_importance_strategy: 'mean'  #[mean, max]
      batch_size: 2
      num_train_steps: 10
      learning_rate: 0.001
      embedding_size: 10
      encoder_layers: 1  # set to 0 for no RNN
      encoder_units: 10
      attn_layers: 2
      attn_units: 10
      classifier_layers: [2, 3, 9, 10, 11]
      classifier_units: 10
      regressor_layers: 1
      regressor_units: 10
      gradient_clip: 5.0
      dropout: 0.4

  - type: 'causal-regression'
    name: 'causal_1'
    skip: True
    params:
      batch_size: 128
      num_train_steps: 10
      learning_rate: 0.01
      reg_type: ['bow', 'all'] # regularize bag of words only or everything
      regularizor: 'l2'   # [l1, l2]
      lambda: 0.1         # 0 turns regularization off
      encoder_layers: [1, 1, 2]
      encoding_dim: [1, 8]
      regression_layers_1: [1, 1, 2]
      regression_hidden_1: [4, 8, 16]
      regression_layers_2: [1, 2]
      regression_hidden_2: [4, 8, 16]
      classification_layers_1: [1, 1, 2]
      classification_hidden_1: [4, 8, 16]
      classification_layers_2: [1, 1, 2]
      classification_hidden_2: [4, 8, 16]

  - type: 'bow-neural'
    name: 'bow-neural'
    skip: True
    params:
      batch_size: 128
      num_train_steps: 10
      learning_rate: 0.001
      reg_type: ['bow', 'all'] # regularize bag of words only or everything
      regularizor: 'l2'   # [l1, l2]
      lambda: 0.1         # 0 turns regularization off
      encoder_layers: [1, 1, 2]
      encoder_units: [1, 8]
      classifier_layers: [1, 1, 2]
      classifier_units: [1, 8, 16]
      regressor_layers: [1, 1, 2]
      regressor_units: [1, 8, 16]
      gradient_clip: 5.0


  - type: "causal-neural"
    name: 'causal_neural_1'
    skip: True
    params:
      use_glove: True. # True, False
      attn_importance_strategy: 'mean'  #[mean, max]
      batch_size: 128
      num_train_steps: 10
      learning_rate: 0.001
      embedding_size: 10
      encoder_layers: 2
      encoder_units: 10
      attn_layers: 2
      attn_units: 10
      classifier_layers: 1
      classifier_units: 10
      regressor_layers: 1
      regressor_units: 10
      gradient_clip: 5.0
      dropout: 0.4


  - type: "mixed-regression"
    name: 'mixed_1'
    skip: True
    params:

  - type: "double-regression"
    name: 'double_1'
    skip: True
    params:
      regularizor: 'l2'   # [l1, l2]
      lambda: 0.0         # 0 turns regularization off
      batch_size: [8, 128, 1000]
      num_train_steps: [1, 10, 50]
      

  - type: "fixed-regression"
    name: 'fixed_1'
    skip: True
    params:
      regularizor: 'l2'   # [l1, l2]
      lambda: 0.0         # 0 turns regularization off
      batch_size: 128
      num_train_steps: 10

  - type: "regression"
    name: 'regression_1'
    skip: True
    params:
      regularizor: 'l2'   # [l1, l2]
      lambda: 0.0         # 0 turns regularization off
      batch_size: 128
      num_train_steps: 1000



######### EVALUATION
num_eval_features: 150


